--- jupyterlab\commands.py (old)
+++ jupyterlab\commands.py (new)
@@ -1,3 +1,4 @@
+# coding: utf-8
 """JupyterLab command handler"""
 
 # Copyright (c) Jupyter Development Team.
@@ -17,20 +18,24 @@
 import subprocess
 import sys
 import tarfile
+import warnings
 from copy import deepcopy
-from dataclasses import dataclass
 from glob import glob
 from pathlib import Path
 from tempfile import TemporaryDirectory
 from threading import Event
-from typing import FrozenSet, Optional
 from urllib.error import URLError
 from urllib.request import Request, quote, urljoin, urlopen
 
 from jupyter_core.paths import jupyter_config_dir
-from jupyter_server.extension.serverextension import GREEN_ENABLED, GREEN_OK, RED_DISABLED, RED_X
+from jupyter_server.extension.serverextension import (
+    GREEN_ENABLED,
+    GREEN_OK,
+    RED_DISABLED,
+    RED_X,
+)
 from jupyterlab_server.config import (
-    get_allowed_levels,
+    LabConfig,
     get_federated_extensions,
     get_package_url,
     get_page_config,
@@ -39,7 +44,7 @@
 )
 from jupyterlab_server.process import Process, WatchHelper, list2cmdline, which
 from packaging.version import Version
-from traitlets import Bool, HasTraits, Instance, List, Unicode, default
+from traitlets import Bool, Dict, HasTraits, Instance, List, Unicode, default
 
 from jupyterlab._version import __version__
 from jupyterlab.coreconfig import CoreConfig
@@ -84,17 +89,15 @@
             The environment for the process.
         """
         if not isinstance(cmd, (list, tuple)):
-            msg = "Command must be given as a list"
-            raise ValueError(msg)
+            raise ValueError("Command must be given as a list")
 
         if kill_event and kill_event.is_set():
-            msg = "Process aborted"
-            raise ValueError(msg)
+            raise ValueError("Process aborted")
 
         self.logger = _ensure_logger(logger)
         self._last_line = ""
         self.cmd = cmd
-        self.logger.debug(f"> {list2cmdline(cmd)}")
+        self.logger.debug("> " + list2cmdline(cmd))
 
         self.proc = self._create_process(
             cwd=cwd,
@@ -119,8 +122,7 @@
             sys.stdout.write("\b")
             if kill_event.is_set():
                 self.terminate()
-                msg = "Process was aborted"
-                raise ValueError(msg)
+                raise ValueError("Process was aborted")
             try:
                 out, _ = proc.communicate(timeout=0.1)
                 cache.append(out)
@@ -192,15 +194,7 @@
     """
     had_dupes = (
         ProgressProcess(
-            [
-                "node",
-                YARN_PATH,
-                "dlx",
-                "yarn-berry-deduplicate",
-                "-s",
-                "fewerHighest",
-                "--fail",
-            ],
+            ["node", YARN_PATH, "yarn-deduplicate", "-s", "fewer", "--fail"],
             cwd=path,
             logger=logger,
         ).wait()
@@ -219,7 +213,7 @@
     """
     logger = _ensure_logger(logger)
     yarn_proc = ProgressProcess(
-        ["node", YARN_PATH, "--immutable", "--immutable-cache"], cwd=cwd, logger=logger
+        ["node", YARN_PATH, "check", "--verify-tree"], cwd=cwd, logger=logger
     )
     ret = yarn_proc.wait()
 
@@ -321,7 +315,7 @@
         startup_regex=WEBPACK_EXPECT,
     )
 
-    return [*package_procs, wp_proc]
+    return package_procs + [wp_proc]
 
 
 class AppOptions(HasTraits):
@@ -337,13 +331,13 @@
         if "app_dir" in kwargs and not kwargs["app_dir"]:
             kwargs.pop("app_dir")
 
-        super().__init__(**kwargs)
+        super(AppOptions, self).__init__(**kwargs)
 
     app_dir = Unicode(help="The application directory")
 
     use_sys_dir = Bool(
         True,
-        help=("Whether to shadow the default app_dir if that is set to a non-default value"),
+        help=("Whether to shadow the default app_dir if that is set to a " "non-default value"),
     )
 
     logger = Instance(logging.Logger, help="The logger to use")
@@ -367,8 +361,6 @@
             " If false, perform a thorough check, which verifies extension contents."
         ),
     )
-
-    verbose = Bool(False, help="Increase verbosity level.")
 
     @default("logger")
     def _default_logger(self):
@@ -416,7 +408,10 @@
     _node_check(app_options.logger)
     handler = _AppHandler(app_options)
 
-    package_procs = watch_packages(app_options.logger) if app_options.splice_source else []
+    if app_options.splice_source:
+        package_procs = watch_packages(app_options.logger)
+    else:
+        package_procs = []
 
     return package_procs + handler.watch()
 
@@ -464,23 +459,22 @@
 def clean(app_options=None):
     """Clean the JupyterLab application directory."""
     app_options = _ensure_options(app_options)
+    handler = _AppHandler(app_options)
     logger = app_options.logger
     app_dir = app_options.app_dir
 
     logger.info("Cleaning %s...", app_dir)
     if app_dir == pjoin(HERE, "dev"):
-        msg = "Cannot clean the dev app"
-        raise ValueError(msg)
+        raise ValueError("Cannot clean the dev app")
     if app_dir == pjoin(HERE, "core"):
-        msg = "Cannot clean the core app"
-        raise ValueError(msg)
+        raise ValueError("Cannot clean the core app")
 
     if getattr(app_options, "all", False):
         logger.info("Removing everything in %s...", app_dir)
         _rmtree_star(app_dir, logger)
     else:
-        possible_targets = ["extensions", "settings", "staging", "static"]
-        targets = [t for t in possible_targets if getattr(app_options, t)]
+        possibleTargets = ["extensions", "settings", "staging", "static"]
+        targets = [t for t in possibleTargets if getattr(app_options, t)]
 
         for name in targets:
             target = pjoin(app_dir, name)
@@ -527,7 +521,7 @@
 
 
 def enable_extension(extension, app_options=None, level="sys_prefix"):
-    """Enable a JupyterLab extension/plugin.
+    """Enable a JupyterLab extension.
 
     Returns `True` if a rebuild is recommended, `False` otherwise.
     """
@@ -536,7 +530,7 @@
 
 
 def disable_extension(extension, app_options=None, level="sys_prefix"):
-    """Disable a JupyterLab extension/plugin.
+    """Disable a JupyterLab package.
 
     Returns `True` if a rebuild is recommended, `False` otherwise.
     """
@@ -548,18 +542,6 @@
     """Check if a JupyterLab extension is enabled or disabled."""
     handler = _AppHandler(app_options)
     return handler.check_extension(extension, installed)
-
-
-def lock_extension(extension, app_options=None, level="sys_prefix"):
-    """Lock a JupyterLab extension/plugin."""
-    handler = _AppHandler(app_options)
-    return handler.toggle_extension_lock(extension, True, level=level)
-
-
-def unlock_extension(extension, app_options=None, level="sys_prefix"):
-    """Unlock a JupyterLab extension/plugin."""
-    handler = _AppHandler(app_options)
-    return handler.toggle_extension_lock(extension, False, level=level)
 
 
 def build_check(app_options=None):
@@ -624,7 +606,7 @@
 # ----------------------------------------------------------------------
 
 
-class _AppHandler:
+class _AppHandler(object):
     def __init__(self, options):
         """Create a new _AppHandler object"""
         options = _ensure_options(options)
@@ -635,26 +617,12 @@
         # Make a deep copy of the core data so we don't influence the original copy
         self.core_data = deepcopy(options.core_config._data)
         self.labextensions_path = options.labextensions_path
-        self.verbose = options.verbose
         self.kill_event = options.kill_event
         self.registry = options.registry
         self.skip_full_build_check = options.skip_full_build_check
 
         # Do this last since it relies on other attributes
         self.info = self._get_app_info()
-        # Migrate from 4.0 which did not have "locked" status
-        try:
-            self._maybe_mirror_disabled_in_locked(level="sys_prefix")
-        except (PermissionError, OSError):
-            try:
-                self.logger.info(
-                    "`sys_prefix` level settings are read-only, using `user` level for migration to `lockedExtensions`"
-                )
-                self._maybe_mirror_disabled_in_locked(level="user")
-            except (PermissionError, OSError):
-                self.logger.warning(
-                    "Both `sys_prefix` and `user` level settings are read-only, cannot auto-migrate `disabledExtensions` to `lockedExtensions`"
-                )
 
     def install_extension(self, extension, existing=None, pin=None):
         """Install an extension package into JupyterLab.
@@ -690,7 +658,7 @@
         # Local directories get name mangled and stored in metadata.
         if info["is_dir"]:
             config = self._read_build_config()
-            local = config.setdefault("local_extensions", {})
+            local = config.setdefault("local_extensions", dict())
             local[name] = info["source"]
             self._write_build_config(config)
 
@@ -738,7 +706,7 @@
         staging = pjoin(app_dir, "staging")
 
         # Make sure packages are installed.
-        ret = self._run(["node", YARN_PATH, "install"], cwd=staging)
+        ret = self._run(["node", YARN_PATH, "install", "--non-interactive"], cwd=staging)
         if ret != 0:
             msg = "npm dependencies failed to install"
             self.logger.debug(msg)
@@ -773,7 +741,7 @@
         )
         return [proc]
 
-    def list_extensions(self):  # noqa
+    def list_extensions(self):
         """Print an output of the extensions."""
         self._ensure_disabled_info()
         logger = self.logger
@@ -796,14 +764,14 @@
         if local:
             logger.info("\n   local extensions:")
             for name in sorted(local):
-                logger.info(f"        {name}: {local[name]}")
+                logger.info("        %s: %s" % (name, local[name]))
 
         linked_packages = info["linked_packages"]
         if linked_packages:
             logger.info("\n   linked packages:")
             for key in sorted(linked_packages):
                 source = linked_packages[key]["source"]
-                logger.info(f"        {key}: {source}")
+                logger.info("        %s: %s" % (key, source))
 
         uninstalled_core = info["uninstalled_core"]
         if uninstalled_core:
@@ -822,7 +790,7 @@
             for item in sorted(disabled):
                 # Show that all plugins will be disabled if the whole extension matches
                 if item in all_exts:
-                    item += " (all plugins)"  # noqa PLW2901
+                    item += " (all plugins)"
                 logger.info("    %s" % item)
 
         # Here check if modules are improperly shadowed
@@ -844,7 +812,7 @@
             logger.info("\nBuild recommended, please run `jupyter lab build`:")
             [logger.info("    %s" % item) for item in messages]
 
-    def build_check(self, fast=None):  # noqa
+    def build_check(self, fast=None):
         """Determine whether JupyterLab should be built.
 
         Returns a list of messages.
@@ -863,7 +831,7 @@
 
         static_data = self.info["static_data"]
         old_jlab = static_data["jupyterlab"]
-        old_deps = static_data.get("dependencies", {})
+        old_deps = static_data.get("dependencies", dict())
 
         # Look for mismatched version.
         static_version = old_jlab.get("version", "")
@@ -878,7 +846,7 @@
         # Look for mismatched extensions.
         new_package = self._get_package_template(silent=fast)
         new_jlab = new_package["jupyterlab"]
-        new_deps = new_package.get("dependencies", {})
+        new_deps = new_package.get("dependencies", dict())
 
         for ext_type in ["extensions", "mimeExtensions"]:
             # Extensions that were added.
@@ -897,7 +865,7 @@
 
         # Look for mismatched dependencies
         src_pkg_dir = pjoin(REPO_ROOT, "packages")
-        for pkg, dep in new_deps.items():
+        for (pkg, dep) in new_deps.items():
             if old_deps.get(pkg, "").startswith(src_pkg_dir):
                 continue
             if pkg not in old_deps:
@@ -910,7 +878,7 @@
                 messages.append(msg % (pkg, old_deps[pkg], new_deps[pkg]))
 
         # Look for updated local extensions.
-        for name, source in local.items():
+        for (name, source) in local.items():
             if fast or name in shadowed_exts:
                 continue
             dname = pjoin(app_dir, "extensions")
@@ -918,7 +886,7 @@
                 messages.append("%s content changed" % name)
 
         # Look for updated linked packages.
-        for name, item in linked.items():
+        for (name, item) in linked.items():
             if fast or name in shadowed_exts:
                 continue
             dname = pjoin(app_dir, "staging", "linked_packages")
@@ -938,7 +906,7 @@
         if name in info["federated_extensions"]:
             if (
                 info["federated_extensions"][name]
-                .get("install", {})
+                .get("install", dict())
                 .get("uninstallInstructions", None)
             ):
                 logger.error(
@@ -966,21 +934,21 @@
 
         local = info["local_extensions"]
 
-        for extname, data in info["extensions"].items():
+        for (extname, data) in info["extensions"].items():
             path = data["path"]
             if extname == name:
-                msg = f"Uninstalling {name} from {osp.dirname(path)}"
+                msg = "Uninstalling %s from %s" % (name, osp.dirname(path))
                 logger.info(msg)
                 os.remove(path)
                 # Handle local extensions.
                 if extname in local:
                     config = self._read_build_config()
-                    data = config.setdefault("local_extensions", {})  # noqa PLW2901
+                    data = config.setdefault("local_extensions", dict())
                     del data[extname]
                     self._write_build_config(config)
                 return True
 
-        logger.warning('No labextension named "%s" installed' % name)
+        logger.warn('No labextension named "%s" installed' % name)
         return False
 
     def uninstall_all_extensions(self):
@@ -989,7 +957,7 @@
         Returns `True` if a rebuild is recommended, `False` otherwise
         """
         should_rebuild = False
-        for extname, _ in self.info["extensions"].items():
+        for (extname, _) in self.info["extensions"].items():
             uninstalled = self.uninstall_extension(extname)
             should_rebuild = should_rebuild or uninstalled
         return should_rebuild
@@ -1000,7 +968,7 @@
         Returns `True` if a rebuild is recommended, `False` otherwise.
         """
         should_rebuild = False
-        for extname, _ in self.info["extensions"].items():
+        for (extname, _) in self.info["extensions"].items():
             if extname in self.info["local_extensions"]:
                 continue
             updated = self._update_extension(extname)
@@ -1025,20 +993,20 @@
         """
         data = self.info["extensions"][name]
         if data["alias_package_source"]:
-            self.logger.warning("Skipping updating pinned extension '%s'." % name)
+            self.logger.warn("Skipping updating pinned extension '%s'." % name)
             return False
         try:
             latest = self._latest_compatible_package_version(name)
         except URLError:
             return False
         if latest is None:
-            self.logger.warning(f"No compatible version found for {name}!")
+            self.logger.warn("No compatible version found for %s!" % (name,))
             return False
         if latest == data["version"]:
             self.logger.info("Extension %r already up to date" % name)
             return False
-        self.logger.info(f"Updating {name} to version {latest}")
-        return self.install_extension(f"{name}@{latest}")
+        self.logger.info("Updating %s to version %s" % (name, latest))
+        return self.install_extension("%s@%s" % (name, latest))
 
     def link_package(self, path):
         """Link a package at the given path.
@@ -1065,7 +1033,7 @@
 
         # Add to metadata.
         config = self._read_build_config()
-        linked = config.setdefault("linked_packages", {})
+        linked = config.setdefault("linked_packages", dict())
         linked[info["name"]] = info["source"]
         self._write_build_config(config)
 
@@ -1080,19 +1048,19 @@
         """
         path = _normalize_path(path)
         config = self._read_build_config()
-        linked = config.setdefault("linked_packages", {})
+        linked = config.setdefault("linked_packages", dict())
 
         found = None
-        for name, source in linked.items():
-            if path in {name, source}:
+        for (name, source) in linked.items():
+            if name == path or source == path:
                 found = name
 
         if found:
             del linked[found]
         else:
-            local = config.setdefault("local_extensions", {})
-            for name, source in local.items():
-                if path in {name, source}:
+            local = config.setdefault("local_extensions", dict())
+            for (name, source) in local.items():
+                if name == path or source == path:
                     found = name
             if found:
                 del local[found]
@@ -1105,108 +1073,32 @@
         self._write_build_config(config)
         return True
 
-    def _is_extension_locked(self, extension, level="sys_prefix", include_higher_levels=True):
-        app_settings_dir = osp.join(self.app_dir, "settings")
-        page_config = get_static_page_config(
-            app_settings_dir=app_settings_dir,
-            logger=self.logger,
-            level=level,
-            include_higher_levels=True,
-        )
-
-        locked = page_config.get("lockedExtensions", {})
-        return locked.get(extension, False)
-
     def toggle_extension(self, extension, value, level="sys_prefix"):
         """Enable or disable a lab extension.
 
         Returns `True` if a rebuild is recommended, `False` otherwise.
         """
-        app_settings_dir = osp.join(self.app_dir, "settings")
-
-        # If extension is locked at a higher level, we don't toggle it.
-        # The highest level at which an extension can be locked is system,
-        # so we do not need to check levels above that one.
-        if level != "system":
-            allowed = get_allowed_levels()
-            if self._is_extension_locked(
-                extension, level=allowed[allowed.index(level) + 1], include_higher_levels=True
-            ):
-                self.logger.info("Extension locked at a higher level, cannot toggle status")
-                return False
-
-        complete_page_config = get_static_page_config(
-            app_settings_dir=app_settings_dir, logger=self.logger, level="all"
-        )
-
-        level_page_config = get_static_page_config(
-            app_settings_dir=app_settings_dir, logger=self.logger, level=level
-        )
-
-        disabled = complete_page_config.get("disabledExtensions", {})
-        disabled_at_level = level_page_config.get("disabledExtensions", {})
-        did_something = False
-        is_disabled = disabled.get(extension, False)
-
-        if value and not is_disabled:
-            disabled_at_level[extension] = True
-            did_something = True
-        elif not value and is_disabled:
-            disabled_at_level[extension] = False
-            did_something = True
-
-        if did_something:
-            level_page_config["disabledExtensions"] = disabled_at_level
-            write_page_config(level_page_config, level=level)
-        return did_something
-
-    def _maybe_mirror_disabled_in_locked(self, level="sys_prefix"):
-        """Lock all extensions that were previously disabled.
-
-        This exists to facilitate migration from 4.0 (which did not include lock
-        function) to 4.1 which exposes the plugin management to users in UI.
-
-        Returns `True` if migration happened, `False` otherwise.
-        """
+        lab_config = LabConfig()
         app_settings_dir = osp.join(self.app_dir, "settings")
 
         page_config = get_static_page_config(
             app_settings_dir=app_settings_dir, logger=self.logger, level=level
         )
-        if "lockedExtensions" in page_config:
-            # short-circuit if migration already happened
-            return False
-
-        # copy disabled onto lockedExtensions, ensuring the mapping format
+
         disabled = page_config.get("disabledExtensions", {})
-        if isinstance(disabled, list):
-            disabled = {extension: True for extension in disabled}
-        page_config["lockedExtensions"] = disabled
-        write_page_config(page_config, level=level)
-        return True
-
-    def toggle_extension_lock(self, extension, value, level="sys_prefix"):
-        """Lock or unlock a lab extension (/plugin)."""
-        app_settings_dir = osp.join(self.app_dir, "settings")
-
-        # The highest level at which an extension can be locked is system,
-        # so we do not need to check levels above that one.
-        if level != "system":
-            allowed = get_allowed_levels()
-            if self._is_extension_locked(
-                extension, level=allowed[allowed.index(level) + 1], include_higher_levels=True
-            ):
-                self.logger.info("Extension locked at a higher level, cannot toggle")
-                return False
-
-        page_config = get_static_page_config(
-            app_settings_dir=app_settings_dir, logger=self.logger, level=level
-        )
-
-        locked = page_config.get("lockedExtensions", {})
-        locked[extension] = value
-        page_config["lockedExtensions"] = locked
-        write_page_config(page_config, level=level)
+        did_something = False
+        is_disabled = disabled.get(extension, False)
+        if value and not is_disabled:
+            disabled[extension] = True
+            did_something = True
+        elif not value and is_disabled:
+            disabled[extension] = False
+            did_something = True
+
+        if did_something:
+            page_config["disabledExtensions"] = disabled
+            write_page_config(page_config, level=level)
+        return did_something
 
     def check_extension(self, extension, check_installed_only=False):
         """Check if a lab extension is enabled or disabled"""
@@ -1217,7 +1109,7 @@
             return self._check_core_extension(extension, info, check_installed_only)
 
         if extension in info["linked_packages"]:
-            self.logger.info(f"{extension}:{GREEN_ENABLED}")
+            self.logger.info("%s:%s" % (extension, GREEN_ENABLED))
             return True
 
         return self._check_common_extension(extension, info, check_installed_only)
@@ -1225,43 +1117,43 @@
     def _check_core_extension(self, extension, info, check_installed_only):
         """Check if a core extension is enabled or disabled"""
         if extension in info["uninstalled_core"]:
-            self.logger.info(f"{extension}:{RED_X}")
+            self.logger.info("%s:%s" % (extension, RED_X))
             return False
         if check_installed_only:
-            self.logger.info(f"{extension}: {GREEN_OK}")
+            self.logger.info("%s: %s" % (extension, GREEN_OK))
             return True
         if extension in info["disabled_core"]:
-            self.logger.info(f"{extension}: {RED_DISABLED}")
+            self.logger.info("%s: %s" % (extension, RED_DISABLED))
             return False
-        self.logger.info(f"{extension}:{GREEN_ENABLED}")
+        self.logger.info("%s:%s" % (extension, GREEN_ENABLED))
         return True
 
     def _check_common_extension(self, extension, info, check_installed_only):
         """Check if a common (non-core) extension is enabled or disabled"""
         if extension not in info["extensions"]:
-            self.logger.info(f"{extension}:{RED_X}")
+            self.logger.info("%s:%s" % (extension, RED_X))
             return False
 
         errors = self._get_extension_compat()[extension]
         if errors:
-            self.logger.info(f"{extension}:{RED_X} (compatibility errors)")
+            self.logger.info("%s:%s (compatibility errors)" % (extension, RED_X))
             return False
 
         if check_installed_only:
-            self.logger.info(f"{extension}: {GREEN_OK}")
+            self.logger.info("%s: %s" % (extension, GREEN_OK))
             return True
 
         if _is_disabled(extension, info["disabled"]):
-            self.logger.info(f"{extension}: {RED_DISABLED}")
+            self.logger.info("%s: %s" % (extension, RED_DISABLED))
             return False
 
-        self.logger.info(f"{extension}:{GREEN_ENABLED}")
+        self.logger.info("%s:%s" % (extension, GREEN_ENABLED))
         return True
 
     def _get_app_info(self):
         """Get information about the app."""
 
-        info = {}
+        info = dict()
         info["core_data"] = core_data = self.core_data
         info["extensions"] = extensions = self._get_extensions(core_data)
 
@@ -1269,7 +1161,7 @@
         info["linked_packages"] = self._get_linked_packages()
         info["app_extensions"] = app = []
         info["sys_extensions"] = sys = []
-        for name, data in extensions.items():
+        for (name, data) in extensions.items():
             data["is_local"] = name in info["local_extensions"]
             if data["location"] == "app":
                 app.append(name)
@@ -1314,11 +1206,6 @@
 
         info["disabled"] = disabled
 
-        locked = page_config.get("lockedExtensions", {})
-        if isinstance(locked, list):
-            locked = {extension: True for extension in locked}
-        info["locked"] = locked
-
         disabled_core = []
         for key in info["core_extensions"]:
             if key in info["disabled"]:
@@ -1326,7 +1213,7 @@
 
         info["disabled_core"] = disabled_core
 
-    def _populate_staging(self, name=None, version=None, static_url=None, clean=False):  # noqa
+    def _populate_staging(self, name=None, version=None, static_url=None, clean=False):
         """Set up the assets in the staging directory."""
         app_dir = self.app_dir
         staging = pjoin(app_dir, "staging")
@@ -1367,7 +1254,7 @@
             target = pjoin(staging, fname)
             shutil.copy(pjoin(source_dir, fname), target)
 
-        for fname in [".yarnrc.yml", "yarn.js"]:
+        for fname in [".yarnrc", "yarn.js"]:
             target = pjoin(staging, fname)
             shutil.copy(pjoin(HERE, "staging", fname), target)
 
@@ -1397,11 +1284,11 @@
         # Update the local extensions.
         extensions = self.info["extensions"]
         removed = False
-        for key, source in self.info["local_extensions"].items():
+        for (key, source) in self.info["local_extensions"].items():
             # Handle a local extension that was removed.
             if key not in extensions:
                 config = self._read_build_config()
-                data = config.setdefault("local_extensions", {})
+                data = config.setdefault("local_extensions", dict())
                 del data[key]
                 self._write_build_config(config)
                 removed = True
@@ -1415,7 +1302,7 @@
 
         # Update the linked packages.
         linked = self.info["linked_packages"]
-        for key, item in linked.items():
+        for (key, item) in linked.items():
             dname = pjoin(staging, "linked_packages")
             self._update_local(key, item["source"], dname, item, "linked_packages")
 
@@ -1463,11 +1350,19 @@
         # copy known-good yarn.lock if missing
         lock_path = pjoin(staging, "yarn.lock")
         lock_template = pjoin(HERE, "staging", "yarn.lock")
-        if not osp.exists(lock_path):
+        if (
+            self.registry != YARN_DEFAULT_REGISTRY
+        ):  # Replace on the fly the yarn repository see #3658
+            with open(lock_template, encoding="utf-8") as f:
+                template = f.read()
+            template = template.replace(YARN_DEFAULT_REGISTRY, self.registry.strip("/"))
+            with open(lock_path, "w", encoding="utf-8") as f:
+                f.write(template)
+        elif not osp.exists(lock_path):
             shutil.copy(lock_template, lock_path)
             os.chmod(lock_path, stat.S_IWRITE | stat.S_IREAD)
 
-    def _get_package_template(self, silent=False):  # noqa
+    def _get_package_template(self, silent=False):
         """Get the template the for staging package.json file."""
         logger = self.logger
         # make a deep copy of the data so we don't influence the core data
@@ -1485,17 +1380,17 @@
                 path = path.lower()
             return path
 
-        jlab["linkedPackages"] = {}
+        jlab["linkedPackages"] = dict()
 
         # Handle local extensions.
-        for key, source in local.items():
+        for (key, source) in local.items():
             if key in shadowed_exts:
                 continue
             jlab["linkedPackages"][key] = source
             data["resolutions"][key] = "file:" + self.info["extensions"][key]["path"]
 
         # Handle linked packages.
-        for key, item in linked.items():
+        for (key, item) in linked.items():
             if key in shadowed_exts:
                 continue
             path = pjoin(self.app_dir, "staging", "linked_packages")
@@ -1504,11 +1399,11 @@
             jlab["linkedPackages"][key] = item["source"]
             data["resolutions"][key] = format_path(path)
 
-        data["jupyterlab"]["extensionMetadata"] = {}
+        data["jupyterlab"]["extensionMetadata"] = dict()
 
         # Handle extensions
         compat_errors = self._get_extension_compat()
-        for key, value in extensions.items():
+        for (key, value) in extensions.items():
             # Reject incompatible extensions with a message.
             errors = compat_errors[key]
             if errors:
@@ -1584,7 +1479,7 @@
     def _get_extensions(self, core_data):
         """Get the extensions for the application."""
         app_dir = self.app_dir
-        extensions = {}
+        extensions = dict()
 
         # Get system level packages.
         sys_path = pjoin(self.sys_dir, "extensions")
@@ -1603,13 +1498,13 @@
 
     def _get_extensions_in_dir(self, dname, core_data):
         """Get the extensions in a given directory."""
-        extensions = {}
+        extensions = dict()
         location = "app" if dname == self.app_dir else "sys"
         for target in glob(pjoin(dname, "extensions", "*.tgz")):
             data = read_package(target)
-            deps = data.get("dependencies", {})
+            deps = data.get("dependencies", dict())
             name = data["name"]
-            jlab = data.get("jupyterlab", {})
+            jlab = data.get("jupyterlab", dict())
             path = osp.abspath(target)
 
             filename = osp.basename(target)
@@ -1618,31 +1513,30 @@
             else:
                 alias = None
             url = get_package_url(data)
-            extensions[alias or name] = {
-                "description": data.get("description", ""),
-                "path": path,
-                "filename": osp.basename(path),
-                "url": url,
-                "version": data["version"],
+            extensions[alias or name] = dict(
+                path=path,
+                filename=osp.basename(path),
+                url=url,
+                version=data["version"],
                 # Only save the package name if the extension name is an alias
-                "alias_package_source": name if alias else None,
-                "jupyterlab": jlab,
-                "dependencies": deps,
-                "tar_dir": osp.dirname(path),
-                "location": location,
-            }
+                alias_package_source=name if alias else None,
+                jupyterlab=jlab,
+                dependencies=deps,
+                tar_dir=osp.dirname(path),
+                location=location,
+            )
         return extensions
 
     def _get_extension_compat(self):
         """Get the extension compatibility info."""
-        compat = {}
+        compat = dict()
         core_data = self.info["core_data"]
         seen = set()
-        for name, data in self.info["federated_extensions"].items():
+        for (name, data) in self.info["federated_extensions"].items():
             deps = data["dependencies"]
             compat[name] = _validate_compatibility(name, deps, core_data)
             seen.add(name)
-        for name, data in self.info["extensions"].items():
+        for (name, data) in self.info["extensions"].items():
             if name in seen:
                 continue
             deps = data["dependencies"]
@@ -1657,18 +1551,18 @@
         """Get the linked packages."""
         info = self._get_local_data("linked_packages")
         dname = pjoin(self.app_dir, "staging", "linked_packages")
-        for name, source in info.items():
-            info[name] = {"source": source, "filename": "", "tar_dir": dname}
+        for (name, source) in info.items():
+            info[name] = dict(source=source, filename="", tar_dir=dname)
 
         if not osp.exists(dname):
             return info
 
         for path in glob(pjoin(dname, "*.tgz")):
-            path = osp.abspath(path)  # noqa PLW2901
+            path = osp.abspath(path)
             data = read_package(path)
             name = data["name"]
             if name not in info:
-                self.logger.warning("Removing orphaned linked package %s" % name)
+                self.logger.warn("Removing orphaned linked package %s" % name)
                 os.remove(path)
                 continue
             item = info[name]
@@ -1707,26 +1601,35 @@
 
         error_accumulator = {}
 
-        logger.info(f"   {ext_type} dir: {dname}")
+        logger.info("   %s dir: %s" % (ext_type, dname))
         for name in sorted(names):
             if name in info["federated_extensions"]:
                 continue
             data = info["extensions"][name]
             version = data["version"]
             errors = info["compat_errors"][name]
-            extra = self._compose_extra_status(name, info, data, errors)
-
+            extra = ""
+            if _is_disabled(name, info["disabled"]):
+                extra += " %s" % RED_DISABLED
+            else:
+                extra += " %s" % GREEN_ENABLED
+            if errors:
+                extra += " %s" % RED_X
+            else:
+                extra += " %s" % GREEN_OK
+            if data["is_local"]:
+                extra += "*"
             # If we have the package name in the data, this means this extension's name is the alias name
             alias_package_source = data["alias_package_source"]
             if alias_package_source:
-                logger.info(f"        {name} {alias_package_source} v{version}{extra}")
+                logger.info("        %s %s v%s%s" % (name, alias_package_source, version, extra))
             else:
-                logger.info(f"        {name} v{version}{extra}")
+                logger.info("        %s v%s%s" % (name, version, extra))
             if errors:
                 error_accumulator[name] = (version, errors)
 
         # Write all errors at end:
-        _log_multiple_compat_errors(logger, error_accumulator, self.verbose)
+        _log_multiple_compat_errors(logger, error_accumulator)
 
         # Write a blank line separator
         logger.info("")
@@ -1738,7 +1641,7 @@
 
         error_accumulator = {}
 
-        ext_dirs = {p: False for p in self.labextensions_path}
+        ext_dirs = dict((p, False) for p in self.labextensions_path)
         for value in info["federated_extensions"].values():
             ext_dirs[value["ext_dir"]] = True
 
@@ -1752,39 +1655,29 @@
                     continue
                 version = data["version"]
                 errors = info["compat_errors"][name]
-                extra = self._compose_extra_status(name, info, data, errors)
+                extra = ""
+                if _is_disabled(name, info["disabled"]):
+                    extra += " %s" % RED_DISABLED
+                else:
+                    extra += " %s" % GREEN_ENABLED
+                if errors:
+                    extra += " %s" % RED_X
+                else:
+                    extra += " %s" % GREEN_OK
+                if data["is_local"]:
+                    extra += "*"
 
                 install = data.get("install")
                 if install:
-                    extra += " ({}, {})".format(install["packageManager"], install["packageName"])
-                logger.info(f"        {name} v{version}{extra}")
+                    extra += " (%s, %s)" % (install["packageManager"], install["packageName"])
+                logger.info("        %s v%s%s" % (name, version, extra))
                 if errors:
                     error_accumulator[name] = (version, errors)
             # Add a spacer line after
             logger.info("")
 
         # Write all errors at end:
-        _log_multiple_compat_errors(logger, error_accumulator, self.verbose)
-
-    def _compose_extra_status(self, name: str, info: dict, data: dict, errors) -> str:
-        extra = ""
-        if _is_disabled(name, info["disabled"]):
-            extra += " %s" % RED_DISABLED
-        else:
-            extra += " %s" % GREEN_ENABLED
-        if errors:
-            extra += " %s" % RED_X
-        else:
-            extra += " %s" % GREEN_OK
-        if data["is_local"]:
-            extra += "*"
-        lock_status = _is_locked(name, info["locked"])
-        if lock_status.entire_extension_locked:
-            extra += " ðŸ”’ (all plugins locked)"
-        elif lock_status.locked_plugins:
-            plugin_list = ", ".join(sorted(lock_status.locked_plugins))
-            extra += " ðŸ”’ (plugins: %s locked)" % plugin_list
-        return extra
+        _log_multiple_compat_errors(logger, error_accumulator)
 
     def _read_build_config(self):
         """Get the build config data for the app dir."""
@@ -1806,16 +1699,16 @@
         """Get the local data for extensions or linked packages."""
         config = self._read_build_config()
 
-        data = config.setdefault(source, {})
+        data = config.setdefault(source, dict())
         dead = []
-        for name, source in data.items():
+        for (name, source) in data.items():
             if not osp.exists(source):
                 dead.append(name)
 
         for name in dead:
             link_type = source.replace("_", " ")
-            msg = f'**Note: Removing dead {link_type} "{name}"'
-            self.logger.warning(msg)
+            msg = '**Note: Removing dead %s "%s"' % (link_type, name)
+            self.logger.warn(msg)
             del data[name]
 
         if dead:
@@ -1844,12 +1737,12 @@
                 try:
                     version = self._latest_compatible_package_version(name)
                 except URLError:
-                    raise ValueError(msg) from None
+                    raise ValueError(msg)
             else:
                 raise ValueError(msg)
 
         # Verify package compatibility.
-        deps = data.get("dependencies", {})
+        deps = data.get("dependencies", dict())
         errors = _validate_compatibility(extension, deps, self.core_data)
         if errors:
             msg = _format_compatibility_errors(data["name"], data["version"], errors)
@@ -1858,13 +1751,13 @@
                     version = self._latest_compatible_package_version(name)
                 except URLError:
                     # We cannot add any additional information to error message
-                    raise ValueError(msg) from None
+                    raise ValueError(msg)
 
                 if version and name:
                     self.logger.debug("Incompatible extension:\n%s", name)
                     self.logger.debug("Found compatible version: %s", version)
                     with TemporaryDirectory() as tempdir2:
-                        return self._install_extension(f"{name}@{version}", tempdir2)
+                        return self._install_extension("%s@%s" % (name, version), tempdir2)
 
                 # Extend message to better guide the user what to do:
                 conflicts = "\n".join(msg.splitlines()[2:])
@@ -1893,7 +1786,7 @@
         if is_dir and not osp.exists(pjoin(source, "node_modules")):
             self._run(["node", YARN_PATH, "install"], cwd=source)
 
-        info = {"source": source, "is_dir": is_dir}
+        info = dict(source=source, is_dir=is_dir)
 
         ret = self._run([which("npm"), "pack", source], cwd=tempdir)
         if ret != 0:
@@ -1911,7 +1804,7 @@
             info["path"] = path
         if pin:
             old_path = info["path"]
-            new_path = pjoin(osp.dirname(old_path), f"{PIN_PREFIX}{pin}.tgz")
+            new_path = pjoin(osp.dirname(old_path), "{}{}.tgz".format(PIN_PREFIX, pin))
             shutil.move(old_path, new_path)
             info["path"] = new_path
 
@@ -1942,12 +1835,13 @@
                 # skip deprecated versions
                 if "deprecated" in data:
                     self.logger.debug(
-                        f"Disregarding compatible version of package as it is deprecated: {name}@{version}"
+                        "Disregarding compatible version of package as it is deprecated: %s@%s"
+                        % (name, version)
                     )
                     continue
                 # Verify that the version is a valid extension.
                 with TemporaryDirectory() as tempdir:
-                    info = self._extract_package(f"{name}@{version}", tempdir)
+                    info = self._extract_package("%s@%s" % (name, version), tempdir)
                 if _validate_extension(info["data"]):
                     # Invalid, do not consider other versions
                     return
@@ -1975,6 +1869,7 @@
                 return _semver_key(key_value[0], prerelease_first=True)
 
             for version, data in sorted(versions.items(), key=sort_key, reverse=True):
+
                 # skip deprecated versions
                 if "deprecated" in data:
                     continue
@@ -1983,14 +1878,14 @@
                 errors = _validate_compatibility(name, deps, core_data)
                 if not errors:
                     # Found a compatible version
-                    keys.append(f"{name}@{version}")
+                    keys.append("%s@%s" % (name, version))
                     break  # break inner for
 
         versions = {}
         if not keys:
             return versions
         with TemporaryDirectory() as tempdir:
-            ret = self._run([which("npm"), "pack", *keys], cwd=tempdir)
+            ret = self._run([which("npm"), "pack"] + keys, cwd=tempdir)
             if ret != 0:
                 msg = '"%s" is not a valid npm package'
                 raise ValueError(msg % keys)
@@ -2030,7 +1925,7 @@
             core_deps = core_data["resolutions"]
             singletons = core_data["jupyterlab"]["singletonPackages"]
 
-            for key, value in latest_deps.items():
+            for (key, value) in latest_deps.items():
                 if key in singletons:
                     # Drop prereleases in comparisons to allow extension authors
                     # to not have to update their versions for each
@@ -2067,8 +1962,7 @@
         Returns the exit code.
         """
         if self.kill_event.is_set():
-            msg = "Command was killed"
-            raise ValueError(msg)
+            raise ValueError("Command was killed")
 
         kwargs["logger"] = self.logger
         kwargs["kill_event"] = self.kill_event
@@ -2080,7 +1974,7 @@
     """Check for the existence of nodejs with the correct version."""
     node = which("node")
     try:
-        output = subprocess.check_output([node, "node-version-check.js"], cwd=HERE)  # noqa S603
+        output = subprocess.check_output([node, "node-version-check.js"], cwd=HERE)
         logger.debug(output.decode("utf-8"))
     except Exception:
         data = CoreConfig()._data
@@ -2089,7 +1983,7 @@
             "Please install nodejs %s before continuing. nodejs may be installed using conda or directly from the nodejs website."
             % ver
         )
-        raise ValueError(msg) from None
+        raise ValueError(msg)
 
 
 def _yarn_config(logger):
@@ -2108,9 +2002,7 @@
 
     try:
         output_binary = subprocess.check_output(
-            [node, YARN_PATH, "config", "--json"],  # noqa S603
-            stderr=subprocess.PIPE,
-            cwd=HERE,
+            [node, YARN_PATH, "config", "list", "--json"], stderr=subprocess.PIPE, cwd=HERE
         )
         output = output_binary.decode("utf-8")
         lines = iter(output.splitlines())
@@ -2132,9 +2024,9 @@
             )
         )
     except Exception as e:
-        logger.error(f"Fail to get yarn configuration. {e!s}")
-
-    return configuration
+        logger.error("Fail to get yarn configuration. {!s}".format(e))
+    finally:
+        return configuration
 
 
 def _ensure_logger(logger=None):
@@ -2177,7 +2069,7 @@
             _rmtree(file_path, logger)
 
 
-def _validate_extension(data):  # noqa
+def _validate_extension(data):
     """Detect if a package is an extension using its metadata.
 
     Returns any problems it finds.
@@ -2189,8 +2081,8 @@
         return ["The `jupyterlab` key must be a JSON object"]
     extension = jlab.get("extension", False)
     mime_extension = jlab.get("mimeExtension", False)
-    theme_path = jlab.get("themePath", "")
-    schema_dir = jlab.get("schemaDir", "")
+    themePath = jlab.get("themePath", "")
+    schemaDir = jlab.get("schemaDir", "")
 
     messages = []
     if not extension and not mime_extension:
@@ -2221,11 +2113,11 @@
     if mime_extension and mime_extension not in files:
         messages.append('Missing mimeExtension module "%s"' % mime_extension)
 
-    if theme_path and not any(f.startswith(str(Path(theme_path))) for f in files):
-        messages.append('themePath is empty: "%s"' % theme_path)
-
-    if schema_dir and not any(f.startswith(str(Path(schema_dir))) for f in files):
-        messages.append('schemaDir is empty: "%s"' % schema_dir)
+    if themePath and not any(f.startswith(str(Path(themePath))) for f in files):
+        messages.append('themePath is empty: "%s"' % themePath)
+
+    if schemaDir and not any(f.startswith(str(Path(schemaDir))) for f in files):
+        messages.append('schemaDir is empty: "%s"' % schemaDir)
 
     return messages
 
@@ -2236,7 +2128,7 @@
     """
     tar = tarfile.open(input_file, "r")
     chunk_size = 100 * 1024
-    h = hashlib.new("sha1")  # noqa: S324
+    h = hashlib.new("sha1")
 
     for member in tar:
         if not member.isfile():
@@ -2266,7 +2158,7 @@
 
     errors = []
 
-    for key, value in deps.items():
+    for (key, value) in deps.items():
         if key in singletons:
             # Drop prereleases in comparisons to allow extension authors
             # to not have to update their versions for each
@@ -2292,7 +2184,7 @@
     return cmp == 0
 
 
-def _compare_ranges(spec1, spec2, drop_prerelease1=False, drop_prerelease2=False):  # noqa
+def _compare_ranges(spec1, spec2, drop_prerelease1=False, drop_prerelease2=False):
     """Test whether two version specs overlap.
 
     Returns `None` if we cannot determine compatibility,
@@ -2377,8 +2269,7 @@
                 return_value = None
             continue
 
-        msg = "Unexpected case comparing version ranges"
-        raise AssertionError(msg)
+        raise AssertionError("Unexpected case comparing version ranges")
 
     if return_value is False:
         return_value = None
@@ -2390,40 +2281,13 @@
     disabled = disabled or {}
     for pattern, value in disabled.items():
         # skip packages explicitly marked as not disabled
-        if value is False:
+        if value == False:
             continue
         if name == pattern:
             return True
         if re.compile(pattern).match(name) is not None:
             return True
     return False
-
-
-@dataclass(frozen=True)
-class LockStatus:
-    entire_extension_locked: bool
-    # locked plugins are only given if extension is not locked as a whole
-    locked_plugins: Optional[FrozenSet[str]] = None
-
-
-def _is_locked(name, locked=None) -> LockStatus:
-    """Test whether the package is locked.
-
-    If only a subset of extension plugins is locked return them.
-    """
-    locked = locked or {}
-    locked_plugins = set()
-    for lock, value in locked.items():
-        # skip packages explicitly marked as not locked
-        if value is False:
-            continue
-        if name == lock:
-            return LockStatus(entire_extension_locked=True)
-        extension_part = lock.partition(":")[0]
-        if name == extension_part:
-            locked_plugins.add(lock)
-
-    return LockStatus(entire_extension_locked=False, locked_plugins=locked_plugins)
 
 
 def _format_compatibility_errors(name, version, errors):
@@ -2446,45 +2310,40 @@
     msg += "Extension".ljust(l1)
     msg += "Package\n"
 
-    for pkg, jlab, ext in msgs:
+    for (pkg, jlab, ext) in msgs:
         msg += jlab.ljust(l0) + ext.ljust(l1) + pkg + "\n"
 
     return msg
 
 
-def _log_multiple_compat_errors(logger, errors_map, verbose: bool):
+def _log_multiple_compat_errors(logger, errors_map):
     """Log compatibility errors for multiple extensions at once"""
 
     outdated = []
-
-    for name, (_, errors) in errors_map.items():
+    others = []
+
+    for name, (version, errors) in errors_map.items():
         age = _compat_error_age(errors)
         if age > 0:
             outdated.append(name)
+        else:
+            others.append(name)
 
     if outdated:
-        logger.warning(
+        logger.warn(
             "\n        ".join(
                 [
-                    "\n   The following extensions may be outdated or specify dependencies that are incompatible with the current version of jupyterlab:",
+                    "\n   The following extensions are outdated:",
                     *outdated,
-                    "\n   If you are a user, check if an update is available for these packages.\n"
-                    + (
-                        "   If you are a developer, re-run with `--verbose` flag for more details.\n"
-                        if not verbose
-                        else "   See below for the details.\n"
-                    ),
+                    '\n   Consider checking if an update is available for these packages.\n',
                 ]
             )
         )
 
-    # Print out compatibility errors for all extensions, even the ones inferred
-    # to be possibly outdated, to guide developers upgrading their extensions.
-    for name, (version, errors) in errors_map.items():
-        if name in outdated and not verbose:
-            continue
+    for name in others:
+        version, errors = errors_map[name]
         msg = _format_compatibility_errors(name, version, errors)
-        logger.warning(f"{msg}\n")
+        logger.warn(msg + "\n")
 
 
 def _log_single_compat_errors(logger, name, version, errors):
@@ -2492,14 +2351,14 @@
 
     age = _compat_error_age(errors)
     if age > 0:
-        logger.warning('The extension "%s" is outdated.\n', name)
+        logger.warn('The extension "%s" is outdated.\n', name)
     else:
         msg = _format_compatibility_errors(name, version, errors)
-        logger.warning(f"{msg}\n")
+        logger.warn(msg + "\n")
 
 
 def _compat_error_age(errors):
-    """Compare all incompatibilities for an extension.
+    """Compare all incompatabilites for an extension.
 
     Returns a number > 0 if all extensions are older than that supported by lab.
     Returns a number < 0 if all extensions are newer than that supported by lab.
@@ -2568,11 +2427,14 @@
     (0.x -> 1.0-pre -> 1.x -> 2.0-pre -> 2.x).
     """
     v = make_semver(version, True)
-    key = ((0,) if v.prerelease else (1,)) if prerelease_first else ()
-    key = (*key, v.major, v.minor, v.patch)
+    if prerelease_first:
+        key = (0,) if v.prerelease else (1,)
+    else:
+        key = ()
+    key = key + (v.major, v.minor, v.patch)
     if not prerelease_first:
         #  NOT having a prerelease is > having one
-        key = (*key, 0) if v.prerelease else (1,)
+        key = key + (0,) if v.prerelease else (1,)
     if v.prerelease:
         key = key + tuple(_semver_prerelease_key(v.prerelease))
 
@@ -2581,10 +2443,12 @@
 
 def _fetch_package_metadata(registry, name, logger):
     """Fetch the metadata for a package from the npm registry"""
-    req = Request(  # noqa S310
+    req = Request(
         urljoin(registry, quote(name, safe="@")),
         headers={
-            "Accept": ("application/vnd.npm.install-v1+json; q=1.0, application/json; q=0.8, */*")
+            "Accept": (
+                "application/vnd.npm.install-v1+json;" " q=1.0, application/json; q=0.8, */*"
+            )
         },
     )
     try:
@@ -2592,7 +2456,7 @@
     except AttributeError:
         logger.debug("Fetching URL: %s" % (req.get_full_url()))
     try:
-        with contextlib.closing(urlopen(req)) as response:  # noqa S310
+        with contextlib.closing(urlopen(req)) as response:
             return json.loads(response.read().decode("utf-8"))
     except URLError as exc:
         logger.warning("Failed to fetch package metadata for %r: %r", name, exc)